# -*- coding: utf-8 -*-
"""3DCNN_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Swethan-colab/Sonu27/blob/main/3DCNN_2.ipynb
"""

import torch
import pandas as pd
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms
import numpy as np
import copy
from sklearn.model_selection import GridSearchCV
from sklearn.base import BaseEstimator, ClassifierMixin

from sklearn.preprocessing import RobustScaler
# Replace 'path/to/my/file.h5' with the actual path to your .h5 file in your Drive
data_path = '/upb/groups/ltm/scratch/Alle/Caylak/Swethan/Ole/Matlab-Code/deep_learning_matlab/PINNs/PINN-CNN/CNN/main/data1000_51voxels.h5'
import h5py

with h5py.File(data_path, 'r') as f:  # Open the file in read-only mode
    # Assuming your RVE data is stored in a dataset named 'rve_data' within the file
    QoIPCAH = f['QoIPCAH'][:]  # Load the data as a NumPy array
    X = f['X'][:]

# Check the data shape to ensure it's 3D (channels, width, height, depth)
print(X.shape,QoIPCAH.shape
 )

from sklearn.model_selection import train_test_split

# Splitting into training and testing sets (excluding validation for now)
X_train_val, X_test_, y_train_val, y_test_ = train_test_split(X, QoIPCAH, test_size=0.1, random_state=42)

# Further split training and validation sets (from X_train_val and y_train_val)


  # Test labels

epochs = 300
batch_size = 50

learning_rate = 0.01

def normalize(data):
  """
  Normalizes data using RobustScaler.

  Args:
      data (torch.Tensor): The data to be normalized.

  Returns:
      torch.Tensor: The normalized data.
  """
  scaler = RobustScaler()
  scaler.fit(data.cpu().numpy())  # Fit scaler on CPU for memory efficiency
  normalized_data = torch.from_numpy(scaler.transform(data.cpu().numpy()))
  normalize_factor = (data/normalized_data)
  return normalized_data,normalize_factor

# Normalize the image data


class My3DCNN(nn.Module):
  def __init__(self):
    super(My3DCNN, self).__init__()
    self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=5, padding=2)
    self.bn1 = nn.BatchNorm3d(16)  # BatchNorm after conv1
    self.relu1 = nn.ReLU()
    self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)

    self.conv2 = nn.Conv3d(in_channels=16, out_channels=16, kernel_size=5, padding=2)
    self.bn2 = nn.BatchNorm3d(16)  # BatchNorm after conv2
    self.relu2 = nn.ReLU()
    self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)

    self.conv3 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=5, padding=2)
    self.bn3 = nn.BatchNorm3d(32)  # BatchNorm after conv3
    self.relu3 = nn.ReLU()
    self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)

    self.flatten = nn.Flatten()
    self.fc1 = nn.Linear(in_features=6912, out_features=128)
    self.relu4 = nn.ReLU()
    self.fc2 = nn.Linear(in_features=128, out_features=64)
    self.relu5 = nn.ReLU()
    self.fc3 = nn.Linear(in_features=64, out_features=6)
    for name, param in self.named_modules():
      if isinstance(param, nn.Conv3d):
        nn.init.kaiming_normal_(param.weight)

  def forward(self, x):
    x = self.bn1(self.relu1(self.conv1(x)))  # Apply BatchNorm after ReLU
    x = self.pool1(x)

    x = self.bn2(self.relu2(self.conv2(x)))  # Apply BatchNorm after ReLU
    x = self.pool2(x)

    x = self.bn3(self.relu3(self.conv3(x)))  # Apply BatchNorm after ReLU
    x = self.pool3(x)

    x = self.flatten(x)
    x = self.relu4(self.fc1(x))
    x = self.relu5(self.fc2(x))
    x = self.fc3(x)
    return x


class PyTorchClassifier(BaseEstimator, ClassifierMixin, nn.Module):
    def __init__(self,physics_weight= 0.3,physics_weight2= 0.2,physics_weight3=0.3, batch_size= 21,learning_rate=0.01, optimizer=torch.optim.Adam):
        super(PyTorchClassifier, self).__init__()  # Call superclass __init__ first
        self.physics_weight = physics_weight
        self.physics_weight2 = physics_weight2
        self.physics_weight3 = physics_weight3
        self.learning_rate = learning_rate
        self.optimizer = optimizer

        self.batch_size= batch_size


    def fit(self,X_train_val, y_train_val):


        X_train = torch.from_numpy(X_train_val).float()  # Train data
        y_train = torch.from_numpy(y_train_val).float()
        X_test = torch.from_numpy(X_test_).float()
        y_test = torch.from_numpy(y_test_).float()  # Train labels
        transform = transforms.Normalize(mean=[0.5], std=[0.5])
        normalized_y_train, normalization_factor_train = normalize(y_train)
        normalized_y_test, normalization_factor_test = normalize(y_test)

        # Create datasets
        train_dataset = TensorDataset(transform(X_train), normalized_y_train, normalization_factor_train)

        test_dataset = TensorDataset(transform(X_test), normalized_y_test, normalization_factor_test)

        # Create dataloaders for efficient batch training
        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        # Create model
        model = My3DCNN()

        def custom_loss(predicted_values, reference_values, x):
            """
            Custom loss function that penalizes negative values and values at indices 4 and 5 that are above 0.5.

            Args:
                predicted_values (torch.Tensor): Tensor of predicted values.
                reference_values (torch.Tensor): Tensor of reference values.

            Returns:
                torch.Tensor: Calculated loss.
            """

            # Calculate mean squared error

            mse_loss = torch.mean((predicted_values - reference_values) ** 2)
            # Calculate penalty for negative values
            negative_penalty = torch.relu(-predicted_values).mean()

            # Calculate penalty for Poisson's ratios exceeding 0.5
            poisson_penalty = torch.relu(predicted_values[:, 4:6] * x[:, 4:6] - 0.5).mean()

            # Combine penalties
            total_penalty = self.physics_weight * negative_penalty + self.physics_weight2 * poisson_penalty

            # Calculate final loss
            loss = mse_loss + self.physics_weight3 * total_penalty

            return loss

        # Create optimizer
        optimizer = self.optimizer(model.parameters(), lr=self.learning_rate)

        # Training loop

        for data, target, norm_factor in train_dataloader:
            optimizer.zero_grad()
            outputs = model(data)
            loss = custom_loss(outputs, target, norm_factor)  # Assuming y_train is not batched
            loss.backward()
            optimizer.step()
        self.model = model
        self.classes_ = np.unique(QoIPCAH)
        return self, test_dataloader


    def predict(self, X_train_val):
        with torch.no_grad():

            X_train = torch.from_numpy(X_train_val).float()
            predictions = self.model(X_train)
        return predictions.numpy()



param_grid = {
    'physics_weight':[0.1,0.3,0.5,1],
    'physics_weight2':[0.1,0.3,0.5,1],
    'physics_weight3':[0.1,0.3,0.5,1],
    'batch_size':[10,20,50],
    'learning_rate': [0.01, 0.001, 0.0001],
    'optimizer':[torch.optim.Adam,torch.optim.AdamW],
}

# Create GridSearchCV object
input_shape = X.shape[1:]
output_shape = QoIPCAH.shape[1]
model1 = PyTorchClassifier()
grid_search = GridSearchCV(estimator=model1, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit GridSearchCV to training data
grid_search.fit(X,QoIPCAH)

# Get best model and parameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_



print("Best Parameters:", best_params)

