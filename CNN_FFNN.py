# -*- coding: utf-8 -*-
"""3DCNN_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Swethan-colab/Sonu27/blob/main/3DCNN_2.ipynb
"""

import torch
import pandas as pd
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms
import numpy as np
import copy
from sklearn.preprocessing import RobustScaler
# Replace 'path/to/my/file.h5' with the actual path to your .h5 file in your Drive
data_path = '/upb/groups/ltm/scratch/Alle/Caylak/Swethan/Ole/Matlab-Code/deep_learning_matlab/PINNs/PINN-CNN/CNN/main/data1000_51voxels.h5'
import h5py

with h5py.File(data_path, 'r') as f:  # Open the file in read-only mode
    # Assuming your RVE data is stored in a dataset named 'rve_data' within the file
    cf = f['fiberVolumeFraction'][:]
    QoIPCAH = f['QoIPCAH'][:]  # Load the data as a NumPy array
    X = f['X'][:]

# Check the data shape to ensure it's 3D (channels, width, height, depth)

from sklearn.model_selection import train_test_split
class MyDataset(torch.utils.data.Dataset):
    def __init__(self, X, cf, qoIPCAH, transform=None, normalize=None, normalize_first_4=None):
        self.X = X
        self.cf = cf
        self.qoIPCAH = qoIPCAH


    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x = self.X[idx].float()
        c = self.cf[idx].float()
        q = self.qoIPCAH[idx].float()



        return x, c, q


transform = transforms.Normalize(mean=[0.5], std=[0.5])
def normalize(data):
  """
  Normalizes data to a range of [0, 1].

  Args:
      data (torch.Tensor): The data to be normalized.

  Returns:
      torch.Tensor: The normalized data.
  """
  min_val = torch.min(data)
  max_val = torch.max(data)
  return (data - min_val) / (max_val - min_val)


def normalize_first_4(data):
    """
    Normalizes only the first 4 values of a tensor using L2 normalization.

    Args:
        data (torch.Tensor): The input tensor.

    Returns:
        torch.Tensor: The normalized tensor.
    """

    # Extract the first 4 values
    data_to_normalize = data[:, :4]

    # Normalize the extracted values using L2 normalization
    normalized_data = torch.nn.functional.normalize(data_to_normalize, p=2, dim=1)

    # Update the first 4 values in the original data
    data[:, :4] = normalized_data

    return data
X= transform(torch.tensor(X))
cf=normalize(torch.tensor(cf))
qoIPCAH_=normalize_first_4(torch.tensor(QoIPCAH))
print(type(qoIPCAH_))
# Assuming you have X, cf, and qoIPCAH already loaded
# First, split into train and test sets
(
    X_train,
    X_test,
    cf_train,
    cf_test,
    qoIPCAH_train,
    qoIPCAH_test,
) = train_test_split(X, cf, qoIPCAH_, test_size=0.1, random_state=42)

# Then, split the train set into train and validation sets
(
    X_train,
    X_val,
    cf_train,
    cf_val,
    qoIPCAH_train,
    qoIPCAH_val,
) = train_test_split(X_train, cf_train, qoIPCAH_train, test_size=0.16, random_state=42)
print(qoIPCAH_val.shape)
train_dataset = MyDataset(X_train, cf_train, qoIPCAH_train, transform=transform, normalize=normalize, normalize_first_4=normalize_first_4)
val_dataset = MyDataset(X_val, cf_val, qoIPCAH_val, transform=transform, normalize=normalize, normalize_first_4=normalize_first_4)
test_dataset = MyDataset(X_test, cf_test, qoIPCAH_test, transform=transform, normalize=normalize, normalize_first_4=normalize_first_4)

# Create data loaders for each split (set your desired batch_size)
batch_size = 16
train_dataloader = torch.utils.data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True
)
val_dataloader = torch.utils.data.DataLoader(
    val_dataset, batch_size=batch_size, shuffle=False
)
test_dataloader = torch.utils.data.DataLoader(
    test_dataset, batch_size=batch_size, shuffle=False
)

epochs = 300


learning_rate = 0.01

# Normalize the image data
transform = transforms.Normalize(mean=[0.5], std=[0.5])

# Create datasets


import torch.nn as nn

class My3DCNN(nn.Module):
    def __init__(self):
        super(My3DCNN, self).__init__()
        # 3D CNN layers (unchanged)
        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm3d(16)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv3d(in_channels=16, out_channels=16, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm3d(16)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)

        self.conv3 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=5, padding=2)
        self.bn3 = nn.BatchNorm3d(32)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)

        # Numerical data branch (cf)
        self.fc_num1 = nn.Linear(in_features=1, out_features=128)
        self.relu_num1 = nn.ReLU()



        # Combined branch
        self.fc_combined = nn.Linear(in_features=128 + 6912, out_features=64)
        self.relu_combined = nn.ReLU()
        self.fc_final = nn.Linear(in_features=64, out_features=6)

        # Initialization (unchanged)
        for name, param in self.named_modules():
            if isinstance(param, nn.Conv3d):
                nn.init.kaiming_normal_(param.weight)

    def forward(self, x_3d, cf):  # Removed qoIPCAH from input
        # 3D CNN branch (unchanged)
        x_3d = self.pool1(self.bn1(self.relu1(self.conv1(x_3d))))
        x_3d = self.pool2(self.bn2(self.relu2(self.conv2(x_3d))))
        x_3d = self.pool3(self.bn3(self.relu3(self.conv3(x_3d))))

        # Numerical data branch (cf)
        cf = self.relu_num1(self.fc_num1(cf))

        # Combine branches (updated)
        x_combined = torch.cat([x_3d.view(-1, 6912), cf], dim=1)
        x_combined = self.relu_combined(self.fc_combined(x_combined))
        x = self.fc_final(x_combined)

        return x

import matplotlib.pyplot as plt

def evaluate(model, val_dataloader, criterion):
    model.eval()
    with torch.no_grad():
        total_val_loss = 0
        for x_3d, cf, qoIPCAH in val_dataloader:
            output = model(x_3d, cf)  # Pass all three inputs
            # Calculate the loss (adapt this based on your task)
            val_loss = criterion(output, qoIPCAH) # Replace target with your desired output
            total_val_loss += val_loss.item()
    return total_val_loss / len(val_dataloader)




def custom_loss(predicted_values, reference_values):
    """
    Custom loss function that penalizes negative values and values at indices 4 and 5 that are above 0.5.

    Args:
        predicted_values (torch.Tensor): Tensor of predicted values.
        reference_values (torch.Tensor): Tensor of reference values.

    Returns:
        torch.Tensor: Calculated loss.
    """

    # Calculate mean squared error
    mse_loss = torch.mean((predicted_values - reference_values) ** 2)

    # Calculate penalty for negative values
    negative_penalty = torch.relu(-predicted_values).mean()

    # Calculate penalty for Poisson's ratios exceeding 0.5
    poisson_penalty = torch.relu(predicted_values[:, 4:6] - 0.5).mean()

    # Combine penalties
    total_penalty = negative_penalty + poisson_penalty

    # Calculate final loss
    loss = mse_loss + total_penalty

    return loss




def train_model(model, train_loader, val_loader, epochs, patience, learning_rate):
    """
    Trains a PyTorch model with early stopping and dropout.

    Args:
        model (nn.Module): The PyTorch model to train.
        train_loader (DataLoader): DataLoader for the training set.
        val_loader (DataLoader): DataLoader for the validation set.
        epochs (int):
 Number of epochs to train.
        patience (int): Number of epochs to wait for improvement before stopping.
        dropout_prob (float): Dropout probability.
        learning_rate (float): Learning rate for the optimizer.

    Returns:
        The trained model and a DataFrame containing training and validation losses.
    """

    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error loss

    training_losses = []
    validation_losses = []
    mse_loss_ = []

    best_val_loss = float('inf')
    best_model = None

    for epoch in range(epochs):
        model.train()  # Set model to training mode
        total_train_loss = 0
        for x_3d, cf, qoIPCAH in val_dataloader:
            output = model(x_3d, cf)
            loss = custom_loss(output, qoIPCAH)

            loss.backward()
            optimizer.step()
            mse_loss_.append(loss.item())

        # Calculate average training loss for the epoch
        avg_mse_loss = sum(mse_loss_) / len(mse_loss_)
        training_losses.append(avg_mse_loss)
        # Evaluate on the validation set

        val_loss = evaluate(model, val_loader,custom_loss)
        validation_losses.append(val_loss)


        # Check for early stopping
        #if val_loss < best_val_loss:
            #best_val_loss = val_loss
            #best_model = copy.deepcopy(model)

         #   patience_counter = 0
        #else:
         #   patience_counter += 1
          #  if patience_counter >= patience:
           #     break

        print(f"Epoch {epoch+1}/{epochs}: Training Loss: {avg_mse_loss:.4f}, Validation Loss: {val_loss:.4f}")

    return best_model, training_losses, validation_losses






model = My3DCNN()
best_model, training_losses, validation_losses = train_model(model, train_dataloader, val_dataloader, epochs, 3, 0.01)
df = pd.DataFrame({'Training Loss': training_losses, 'Validation Loss': validation_losses})
df.to_excel('training_validation_losses.xlsx', index=False)

# Assuming you have a trained PyTorch model named 'model'

param_grid = {
    'batch_size': batch_size,
    'epochs': epochs,
    'optimizer_name': 'AdamW',  # Assuming AdamW is the optimizer
    'learning_rate': learning_rate
}
model_state_dict = model.state_dict()
all_state = {
    'model_state_dict': model_state_dict,
    'param_grid': param_grid
}

torch.save(all_state, 'model1_1000_0.01lr.pth')

# Convert to NumPy arrays



