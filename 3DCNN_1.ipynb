{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swethan-colab/Sonu27/blob/main/3DCNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LUH1vRQHYcq1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'path/to/my/file.h5' with the actual path to your .h5 file in your Drive\n",
        "data_path = '/content/drive/MyDrive/data1000_51voxels.h5'\n",
        "import h5py\n",
        "\n",
        "with h5py.File(data_path, 'r') as f:  # Open the file in read-only mode\n",
        "    # Assuming your RVE data is stored in a dataset named 'rve_data' within the file\n",
        "    QoIPCAH = f['QoIPCAH'][:]  # Load the data as a NumPy array\n",
        "    X = f['X'][:]\n",
        "\n",
        "# Check the data shape to ensure it's 3D (channels, width, height, depth)\n",
        "print(X.shape,QoIPCAH.shape\n",
        " )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ZTBJWTZBGV",
        "outputId": "21f5c23a-e41a-4efc-96e0-8edc63ddbc64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 1, 51, 51, 51) (1000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pKQYqMC9YfXh"
      },
      "outputs": [],
      "source": [
        "# Assuming data.X has shape (num_samples, 51, 51, 51, 1)\n",
        "# Assuming data.QoIPCAH has shape (num_samples, 6)\n",
        "\n",
        "train_split = 750\n",
        "val_split=150\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train = torch.from_numpy(X[:train_split]).float()  # Train data\n",
        "X_val = torch.from_numpy(X[train_split:val_split]).float()  # Validation data\n",
        "X_test = torch.from_numpy(X[val_split:]).float()  # Test data\n",
        "\n",
        "print(f)\n",
        "\n",
        "y_train = torch.from_numpy(QoIPCAH[:train_split]).float()  # Train labels\n",
        "y_val = torch.from_numpy(QoIPCAH[train_split:val_split]).float()  # Validation labels\n",
        "y_test = torch.from_numpy(QoIPCAH[val_split:]).float()  # Test labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "patience = 5  # Early stopping patience\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "UEdArz4Y4IW7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the image data (optional)\n",
        "transform = transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(transform(X_train), y_train)\n",
        "val_dataset = TensorDataset(transform(X_val), y_val)\n",
        "test_dataset = TensorDataset(transform(X_test), y_test)\n",
        "batch_size=32\n",
        "data_sample, target = train_dataset[0]  # Access first element\n",
        "\n",
        "print(data_sample.shape)  # Check the shape of the data sample (e.g., torch.Size([1, 3, 32, 32, 32]))\n",
        "print(target)\n",
        "\n",
        "# Create dataloaders for efficient batch training\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "mkcdAlCzaKjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909dbb0f-0f9c-40d7-a6c9-a54ab83bc977"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 51, 51, 51])\n",
            "tensor([162.4926, 103.6210,  38.8113,  41.1782,   0.2302,   0.3349])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class My3DCNN(nn.Module):\n",
        "  def __init__(self, input_shape, output_size):\n",
        "    super(My3DCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=5, padding=2)\n",
        "    self.bn1 = nn.BatchNorm3d(16)  # BatchNorm after conv1\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv2 = nn.Conv3d(in_channels=16, out_channels=16, kernel_size=5, padding=2)\n",
        "    self.bn2 = nn.BatchNorm3d(16)  # BatchNorm after conv2\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv3 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
        "    self.bn3 = nn.BatchNorm3d(32)  # BatchNorm after conv3\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(in_features=6912, out_features=128)\n",
        "    self.relu4 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
        "    self.relu5 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(in_features=64, out_features=output_size)\n",
        "    for name, param in self.named_modules():\n",
        "      if isinstance(param, nn.Conv3d):\n",
        "        nn.init.kaiming_normal_(param.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(self.relu1(self.conv1(x)))  # Apply BatchNorm after ReLU\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.bn2(self.relu2(self.conv2(x)))  # Apply BatchNorm after ReLU\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.bn3(self.relu3(self.conv3(x)))  # Apply BatchNorm after ReLU\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.relu4(self.fc1(x))\n",
        "    x = self.relu5(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "1WUAgnI4aghb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()  # Set model to evaluation mode (optional for some layers)\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for data, target in data_loader:\n",
        "            output = model(data)\n",
        "            val_loss = criterion(output, target)\n",
        "            total_val_loss += val_loss.item()\n",
        "    return total_val_loss / 250\n",
        "def train_model(model, train_loader, val_loader, epochs, patience):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # Adjust optimizer\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        # Train loop\n",
        "        total_train_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            optimizer.zero_grad()  # Reset gradients for each batch\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target) + get_regularization_loss(model)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Calculate and print average training loss for the epoch\n",
        "        avg_train_loss = total_train_loss / 750\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        val_loss = evaluate(model, val_dataloader, criterion)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())  # Save best weights\n",
        "            patience = 5  # Reset patience on improvement\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        if patience == 0:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "k9f37Tm03ph8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1_lambda = 0.001  # Adjust this hyperparameter for L1 regularization\n",
        "l2_lambda = 0.005  # Adjust this hyperparameter for L2 regularization\n",
        "\n",
        "def get_regularization_loss(model):\n",
        "  # Iterate through all model parameters and sum their L1 or L2 norm\n",
        "  regularization_loss = 0\n",
        "  for param in model.parameters():\n",
        "    if param.requires_grad:  # Only consider parameters with gradients\n",
        "      regularization_loss += l1_lambda * torch.abs(param).sum()  # L1 regularization\n",
        "      # OR\n",
        "      regularization_loss += l2_lambda * torch.sum(torch.square(param))  # L2 regularization\n",
        "  return regularization_loss"
      ],
      "metadata": {
        "id": "t_vKziRASNK1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X.shape[1:]\n",
        "output_shape = QoIPCAH.shape[1]\n",
        "model = My3DCNN(input_shape,output_shape)\n",
        "#clip_value = 0.005  # Experiment with different clip values\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)  # Stochastic Gradient Descent optimizer\n"
      ],
      "metadata": {
        "id": "rbFrxg6Zasjx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model,train_dataloader,val_dataloader,30,5)"
      ],
      "metadata": {
        "id": "5CwTfaVGeqsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcea290e-b00d-4022-85a2-3fe3f6f7feef"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 103.6369\n",
            "Val Loss: 0.0000\n",
            "Epoch 2/30, Train Loss: 4.0803\n",
            "Val Loss: 0.0000\n",
            "Epoch 3/30, Train Loss: 0.6748\n",
            "Val Loss: 0.0000\n",
            "Epoch 4/30, Train Loss: 0.4623\n",
            "Val Loss: 0.0000\n",
            "Epoch 5/30, Train Loss: 0.4230\n",
            "Val Loss: 0.0000\n",
            "Epoch 6/30, Train Loss: 0.4071\n",
            "Early stopping at epoch 6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "My3DCNN(\n",
              "  (conv1): Conv3d(1, 16, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
              "  (bn1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu1): ReLU()\n",
              "  (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv3d(16, 16, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
              "  (bn2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu2): ReLU()\n",
              "  (pool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv3d(16, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
              "  (bn3): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu3): ReLU()\n",
              "  (pool3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=6912, out_features=128, bias=True)\n",
              "  (relu4): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (relu5): ReLU()\n",
              "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1W7chkxnaOVhEZWDS3hqiJb-9MF7ePJVv",
      "authorship_tag": "ABX9TyORx66WzNMNofEmQtV73fOQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}